---
layout: post
title: rLLM is Released Now!
subtitle: rLLM(relationLLM) is released for Relational Table Learning with LLMs
gh-repo: rllm-team/rllm
gh-badge: [star, fork, follow]
tags: [release]
mathjax: true
author: rLLM Group
# cover-img: /assets/img/2024-10-09/bkgd.jpg
# thumbnail-img: /assets/img/2024-10-09/bkgd.jpg
---

Recently, our research team at Shanghai Jiao Tong University and Tsinghua University released the first version of **rLLM** (relationLLM). The goal is to facilitate the machine learning workflows on those data stored in the relational databases with Large Language Models (LLMs).

![rllm_overview](/assets/img/2024-10-10/rllm_overview.webp){: .mx-auto.d-block :}

As shown above, rLLM performs two key functions:

1) Breaking down state-of-the-art GNNs, LLMs, and TNNs as standardized modules.

2) Enabling the construction of novel models in a “combine, align, and co-train” way with these decomposed modules.
---
<br>

To learn more:

* Paper ([Slides](https://zhengwang100.github.io/pdf/rllm_introduction240811.pdf)): [https://arxiv.org/abs/2407.20157](https://arxiv.org/abs/2407.20157).

* Code: [https://github.com/rllm-project/rllm](https://github.com/rllm-project/rllm).